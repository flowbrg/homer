\documentclass[11pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{enumitem}
\usepackage{url}

% Page geometry
\geometry{
    left=2.5cm,
    right=2.5cm,
    top=3cm,
    bottom=3cm
}

\setlength{\parindent}{0pt}

% Header and footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{Homer Technical Report}
\fancyhead[R]{SCK CEN}
\fancyfoot[C]{\thepage}
\renewcommand{\headrulewidth}{0.4pt}

% Code listing style
\lstdefinestyle{pythonstyle}{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numberstyle=\tiny\color{gray},
    numbers=left,
    stepnumber=1,
    numbersep=8pt,
    showstringspaces=false,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    captionpos=b
}

\lstset{style=pythonstyle}

% Hyperref setup
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,
    urlcolor=cyan,
    pdftitle={Homer Technical Report},
    pdfauthor={Florent Bergé, Mathieu de la Barre},
    pdfsubject={RAG Application for Nuclear Reactor Documentation},
    pdfkeywords={RAG, AI, Nuclear, MYRRHA, LangGraph, ChromaDB}
}

% Title page information
\title{\textbf{Homer - Technical Report}}
\author{
    Florent Bergé \\
    \and
    Mathieu de la Barre \\
    \textit{IMT Atlantique}
}
\date{March 1st - July 18th, 2025}

\begin{document}

% Title page
\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\huge\bfseries Homer - Technical Report\par}
    \vspace{1cm}
    {\Large\itshape A Retrieval-Augmented Generation System for Nuclear Engineers Documentation\par}
    \vspace{2cm}
    
    {\Large
    Florent Bergé \\
    Mathieu de la Barre \\
    \textit{IMT Atlantique}
    \par}
    \vspace{1cm}
    
    {\large Supervised by: Mr. Vincenzo De Florio\par}
    \vspace{2cm}
    
    {\large
    \textbf{SCK CEN} \\
    Belgian Nuclear Research Centre \\
    April 1st - July 18th, 2025
    \par}
    
    \vfill
    
    {\large
    Classification: Restricted
    \par}
    
\end{titlepage}

% Table of contents
\tableofcontents
\newpage

\section{Abstract}

This report presents the results of a research and development project conducted by Florent Bergé and Mathieu de la Barre, engineering trainees from IMT Atlantique, between March 1st and July 18th, 2025. The project focused on the design and implementation of Homer, a generative AI assistant tailored for the retrieval and interpretation of technical knowledge within the framework of the SMR-LFR nuclear reactor program at SCK CEN.

The work was carried out under the supervision of Mr. Vincenzo De Florio, and builds on existing specifications, which are assumed as prior knowledge for the reader. The report details both the conceptual foundations and the technical implementations of the assistant, from document analysis and vectorization strategies to evaluation protocols and system architecture. The resulting tool is expected to significantly improve information accessibility and decision-making efficiency for domain experts engaged in advanced nuclear research.

\section{Introduction}

The SCK CEN has progressively accumulated a substantial body of knowledge through its involvement in the MYRRHA program and, more recently, the SMR-LFR nuclear reactor initiative. This expertise has been systematically documented in the Conceptual Design Report (CDR), a dynamic reference work currently exceeding 3,500 pages, as well as in an extensive knowledge base of requirements, design rationales, and engineering constraints. These resources are managed within Polarion environments dedicated to Requirements and Configuration Management.

In light of the growing complexity and scale of these assets, SCK CEN has initiated an exploration into the use of generative AI technologies. The goal is to develop a reasoning assistant—akin to an expert system—that can support engineers in efficiently retrieving critical information related to MYRRHA and in navigating toward the most pertinent enterprise knowledge resources.

The result of this initiative is Homer, a multi-user assistant based on Retrieval-Augmented Generation (RAG) architecture. Designed as a secure, cloud-hosted solution within SCK CEN's infrastructure, it is tailored to respect the confidentiality and sensitivity of the internal documentation.

The name Homer pays homage to the ancient Greek poet traditionally regarded as the author of the Iliad and the Odyssey. In conceptual harmony with Alexandria—a symbolic reference to the accumulation and preservation of knowledge—Homer stands for its interpretation and transmission\cite{berge2025specifications}.

\newpage
\chapter{Concepts}

\section{Retrieval-Augmented Generation}

The Retrieval-Augmented Generation (RAG) pattern is an industry-standard approach to building applications that use language models to process specific or proprietary data that the model doesn't already know. The architecture is straightforward and best suited for large and fast-changing data sets (compared to fine-tuning).

\begin{figure}[H]
        \centering
        \includegraphics[width=1\linewidth]{static/schemas/query.png}
        \caption{RAG application flow}
        \label{fig:ragflow}
    \end{figure}

\begin{enumerate}
    \item The user issues a query
    \item The application performs a search on the vectorstore based on the user input
    \item The top k results matching the query are extracted. Both the results and the query are then packaged together in a prompt and sent to the Large Language Model (LLM)
    \item The LLM outputs a structured answer based on both the input and the retrieved context
\end{enumerate}

\section{Core Components}

\subsection{Vectorstore}
A vectorstore is a specialized database designed to store and retrieve high-dimensional vectors efficiently. In the context of RAG applications, vectorstores serve as the foundation for semantic search capabilities. They enable the system to find relevant documents based on meaning rather than exact keyword matches. The vectorstore indexes document embeddings and supports similarity searches using distance metrics such as cosine similarity, Euclidean distance, or dot product. Popular vectorstore solutions include Pinecone, Weaviate, Chroma, and FAISS, each offering different trade-offs between performance, scalability, and ease of deployment.

\subsection{Tokens, Embeddings and Embedding Models}
\textbf{Tokens} are the fundamental units of text that language models process. Text is broken down into tokens, which can represent words, subwords, or characters depending on the tokenization strategy. Modern models typically use subword tokenization methods like Byte Pair Encoding (BPE) or SentencePiece.

\textbf{Embeddings} are dense vector representations of text that capture semantic meaning in a high-dimensional space. Each embedding is a numerical array where semantically similar concepts are positioned closer together in the vector space. These embeddings enable the system to understand context and meaning beyond simple keyword matching.

\textbf{Embedding models} are specialized neural networks trained to convert text into these vector representations. Popular embedding models include OpenAI's text-embedding-ada-002, Sentence-BERT, and various models from Hugging Face's transformers library. The choice of embedding model significantly impacts the quality of document retrieval and overall system performance.

\subsection{Chat Model}
The chat model is the large language model responsible for generating responses in the RAG system. It receives both the user's query and the retrieved context documents as input, then synthesizes this information to produce coherent, contextually relevant answers. Common chat models include GPT-3.5/4, Claude, Llama, and other instruction-tuned language models. The chat model's performance depends on its ability to understand context, follow instructions, and generate accurate responses based on the provided information.

\section{Document Processing Pipeline}

\subsection{Indexing}
An indexing pipeline allows transforming files into useable data for the RAG application. The data pipeline processes each document individually by completing the following steps:

\begin{figure}[H]
        \centering
        \includegraphics[width=1\linewidth]{static/schemas/documentLoader.png}
        \caption{Indexing pipeline}
        \label{fig:indexpipeline}
\end{figure}

\begin{enumerate}
    \item \textbf{Loading}: Data is extracted from the documents
    \item \textbf{Chunking}: The document is divided into smaller parts—recommended between 256 and 512 chunks—according to a chunking technique
    \item \textbf{Chunk enrichment}: Adds metadata fields that the pipeline creates based on the content in the chunks, such as the source document, the titles and other relevant information
    \item \textbf{Indexing}: All the chunks are turned into embeddings and stored into a vector database according to an indexing algorithm\cite{berge2025hybrid}
\end{enumerate}

\subsection{Chunking Techniques}
The system supports two chunking approaches:

\textbf{Recursive Character Splitting}: The data is divided into chunks of a given size with specified overlap between adjacent chunks. The overlap prevents sentences from being split in the middle, which could potentially cause loss of meaning. This method provides consistent chunk sizes but may not preserve semantic boundaries.

\textbf{Semantic Text Chunking}: This technique requires an embedding model to divide content based on meaning rather than character count. It preserves most sentences or propositions intact and then combines semantically related chunks together. While more computationally expensive, this approach often results in more coherent and contextually meaningful chunks.

\section{Framework Architecture}

\subsection{LangChain Framework}
LangChain is a popular open-source framework designed to simplify the development of applications powered by large language models. It provides a comprehensive set of tools and abstractions that streamline the implementation of RAG systems by offering pre-built components, standardized interfaces, and seamless integrations with various LLMs, vector databases, and data sources. The framework follows a modular architecture that allows developers to mix and match components while maintaining clean, maintainable code.

\textbf{Key LangChain Components:}

\begin{itemize}
    \item \textbf{The Retriever}: A core abstraction that defines a standard interface for fetching relevant documents based on a query. It acts as the bridge between the user's question and the vectorstore, encapsulating the search logic and returning a list of relevant documents
    \begin{itemize}
        \item \textit{VectorStoreRetriever}: Performs similarity search on vector databases
        \item \textit{MultiQueryRetriever}: Generates multiple variations of the user query to improve retrieval coverage
    \end{itemize}
    
    \item \textbf{The Document Class}: LangChain's standardized representation of a piece of text content along with its associated metadata. Each Document object contains:
    \begin{itemize}
        \item \textit{page\_content}: The actual text content of the document chunk
        \item \textit{metadata}: A dictionary containing additional information such as source file, page number, creation date, document type, and custom fields
    \end{itemize}
\end{itemize}

\subsection{LangGraph}
LangGraph is a framework built on top of LangChain that enables the creation of stateful, multi-actor applications with large language models. It represents a paradigm shift from simple linear chains to complex, graph-based workflows where different components can interact, make decisions, and maintain state over time. LangGraph is particularly valuable for RAG applications that require sophisticated orchestration, conditional logic, human-in-the-loop workflows, or multi-step reasoning processes.

\textbf{Core LangGraph Components:}

\begin{itemize}
    \item \textbf{The Graph}: The core abstraction that defines the structure and flow of your application. It represents a directed graph where nodes perform specific tasks and edges define the transitions between these tasks;
    \item \textbf{Nodes}: Individual processing units within a LangGraph. Each node encapsulates a specific function, receiving the current state as input and returning updated state information;
    \item \textbf{State Management}: Allows information to persist and evolve throughout the execution of a graph. The state serves as a shared data structure that nodes can read from and write to;
    \item \textbf{The Checkpointer}: Responsible for persisting the state of a graph execution at specific points, enabling durability and recovery capabilities. In our implementation, we use an in-memory SQLite database;
    \begin{lstlisting}[caption={Checkpointer usage}]
from langgraph.checkpoint.sqlite import SqliteSaver

builder = StateGraph(...

conn = sqlite3.connect(':memory:', check_same_thread=False)
memory = SqliteSaver(conn)

graph = builder.compile(checkpointer=memory)
    \end{lstlisting}
    \item \textbf{The Runnable Config}: A configuration object that controls how a graph executes, providing fine-grained control over the execution environment and allowing configuration to be passed inside the nodes. In order to provide a custom configuration to the graph, it needs to be passed in the "configurable" argument of the RunnableConfig and turned into a dictionary. To use a checkpoint, you also have to add a \textit{thread\_id} to the configurable dictionary;
    \begin{lstlisting}[caption={Runnable config usage}]
configuration = {"parameter1":value1, ...}|{"thread_id":1}

graph.invoke(
    input=user_input,
    config={"configurable":configuration}
)
    \end{lstlisting}
\end{itemize}

\chapter{Architecture}

Homer is a RAG application that allows users to provide their own documents to augment the agent's knowledge. It is composed of a Streamlit web interface, LangGraph agents, and an Ollama client.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{static/schemas/scope.png}
    \label{fig:scope}
\end{figure}

The core Application (interface, agents and vectorstore) runs on the local machine and sends requests to an Ollama client which handles model inferences. The latter can either be running on the local machine as well or on a distant server.

This architecture was chosen because it allows confidential data to stay on the local machine. Indeed, having a shared database between users would have required either managing the access of each user to specific documents or creating user-specific spaces which would have caused redundancy. Hence, the decision was made to perform all small calculations on the local computer and let a server run the models using the Ollama API.

\section{Code architecture}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{static/schemas/projectOverview.drawio.png}
    \caption{Code organization}
    \label{fig:overview}
\end{figure}

The application starts by running:

\begin{lstlisting}
python main.py
\end{lstlisting}

This file ensures mandatory directories exist and then runs the line:

\begin{lstlisting}
streamlit run streamlit_app.py
\end{lstlisting}

\begin{itemize}
    \item \textbf{Pages} refers to the streamlit interface pages, which are managed by the file \texttt{streamlit\_app}
    \item \textbf{core} refers to the core components (configuration and agents related files)
    \item \textbf{utils} are all the utility functions that can be called in any file, such as the log management, functions to load a sqlite3 connection etc
    \item \textbf{parser} refers to the custom pdf parser described in Chapter 3
\end{itemize}

\section{Streamlit Client}

The Streamlit client is composed of 6 pages:

\begin{itemize}
    \item \textbf{Home}: Welcome page with navigation
    \item \textbf{Discussion}: The page where users can chat with the RetrievalAgent
    \item \textbf{Report}: The page where users can send a query to the ReportAgent
    \item \textbf{Documents}: The page to add documents to the knowledge base
    \item \textbf{Configuration}: The page where users can temporarily modify parameters
    \item \textbf{Models}: Allows users to enter an Ollama model to pull
\end{itemize}

\subsection*{Session state}

Streamlit uses a 'session\_state' to store session variables such as:

\begin{itemize}
    \item \textbf{baseConfig} (Configuration): Loads the default Configuration
    \item \textbf{ollama\_host} (str): The IP address of the distant Ollama client
    \item \textbf{models} (dictionary): The discussion models, reasoning or standard, local or server (server ones are larger and more accurate than local ones)
\end{itemize}

\subsection{Discussion Page}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{static/figures/discussionPage.png}
    \label{fig:discussionPage}
\end{figure}

This is the main page for asking questions to the RetrievalAgent. The discussion displays in the main page, above the input field at the bottom. It features 2 buttons in the sidebar:

\begin{itemize}
    \item \textbf{Server Execution}: Allows running models on the distant client at the 'ollama\_host' address, if available
    \item \textbf{Reasoning Model}: Allows switching between traditional and reasoning models. Due to their 'thinking' step, they take more time to output an answer. When Reasoning model is on, the thinking part will be accessible through an expander on top of the answer
\end{itemize}

The specific session\_state values for this page are:
\begin{itemize}
    \item \textbf{retrievalAgent} (RetrievalAgent): The compiled retrieval agent
    \item \textbf{ollama\_host}
    \item \textbf{baseConfig}
    \item \textbf{models}
\end{itemize}

Loading the retrieval agent in the session\_state allows compiling the graph only once.

\subsection{Report Page}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{static/figures/reportPage.png}
    \label{fig:reportPage}
\end{figure}

This page allows the user to request a report to the ReportAgent. The sidebar features 2 options:

\begin{itemize}
    \item \textbf{Server Execution}
    \item \textbf{Writing}: Allows switching between 'general' and 'technical' writing style
\end{itemize}

Once a report is completed, a button to download it.

The specific session\_state values for this page are:
\begin{itemize}
    \item \textbf{reportAgent} (ReportAgent): The compiled report agent
    \item \textbf{report\_history} (list): A list of previous reports from the session
    \item \textbf{ollama\_host}
    \item \textbf{baseConfig}
\end{itemize}

\subsection{Document Page}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{static/figures/documentPage.png}
    \label{fig:documentPage}
\end{figure}

This page allows the user to send documents to the documents pipeline (IndexAgent). The sidebar features 2 options:

\begin{itemize}
    \item \textbf{Server Execution}
    \item \textbf{Vision parser}: Allows switching between PyMuPDF parser and a custom Vision parser (using a LLVM)
\end{itemize}

The specific session\_state values for this page are:
\begin{itemize}
    \item \textbf{indexAgent} (IndexAgent): The compiled indexing pipeline
    \item \textbf{ollama\_host}
    \item \textbf{baseConfig}
\end{itemize}

\subsection{Model Page}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{static/figures/modelPage.png}
    \label{fig:modelPage}
\end{figure}

This page allows the user to pull a model to either the local or server ollama client.

\section{Configuration Management}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{static/schemas/classDiagramConfiguration.drawio.png}
    \label{fig:configuration}
\end{figure}

\subsection{Configuration Architecture}

Homer implements a centralized configuration system through the \texttt{Configuration} dataclass located in \texttt{src/core/configuration.py}. This approach ensures type safety, validation, and consistent parameter management across all system components.

The configuration system utilizes Python's \texttt{dataclass} decorator with the \texttt{kw\_only=True} parameter, enforcing explicit parameter naming and improving code readability. The implementation supports both static default values and dynamic configuration loading based on Ollama client availability.

\begin{lstlisting}[caption={Configuration dataclass structure}]
@dataclass(kw_only=True)
class Configuration:
    # Report configuration
    number_of_parts: int = field(default=5)
    
    # Ollama configuration  
    ollama_host: str = field(default=OLLAMA_LOCALHOST)
    
    # Model specifications
    embedding_model: str = field(default="nomic-embed-text")
    response_model: str = field(default="gemma3:1b")
    # ... additional model configurations
\end{lstlisting}

\subsection{Dynamic Host Resolution}

The system implements intelligent host resolution through the \texttt{\_is\_ollama\_client\_available()} function, which performs connectivity checks with a 2-second timeout. When the remote Ollama client at \texttt{OLLAMA\_CLIENT} is available, the configuration automatically switches to server execution mode. Otherwise, it falls back to local execution using \texttt{OLLAMA\_LOCALHOST} (\texttt{http://127.0.0.1:11434/}).

This dual-mode operation enables:
\begin{itemize}
    \item \textbf{Local development} with reduced model capabilities but full functionality
    \item \textbf{Production deployment} with access to larger, more capable models on dedicated hardware
    \item \textbf{Automatic failover} ensuring system availability regardless of server status
\end{itemize}

\subsection{Configuration Integration with LangGraph}

The configuration system integrates seamlessly with LangGraph's \texttt{RunnableConfig} through the \texttt{from\_runnable\_config()} class method. This enables:

\begin{lstlisting}[caption={Configuration integration with LangGraph}]
configuration = Configuration.from_runnable_config(config)
configurable = config.get("configurable") or {}
\end{lstlisting}

Each graph node receives configuration parameters through the standardized LangGraph configuration mechanism, ensuring consistent behavior across all agents and maintaining separation of concerns between business logic and configuration management.

\subsection{Model Selection Strategy}

Homer implements a sophisticated model selection strategy supporting four distinct model categories. These are the defaults:

\begin{table}[H]
\centering
\begin{tabular}{@{}llll@{}}
\toprule
\textbf{Category} & \textbf{Local Model} & \textbf{Server Model} & \textbf{Use Case} \\
\midrule
Standard & \texttt{gemma3:1b} & \texttt{gemma3:4b-it-qat} & General responses \\
Reasoning & \texttt{qwen3:0.6b} & \texttt{qwen3:30b-a3b} & Complex analysis \\
Embedding & \texttt{nomic-embed-text} & \texttt{nomic-embed-text} & Document vectorization \\
Vision & N/A & \texttt{qwen2.5vl:3b-q4\_K\_M} & PDF parsing with OCR \\
\bottomrule
\end{tabular}
\caption{Model selection strategy across different categories}
\label{tab:model-selection}
\end{table}

These values can be modified in the \texttt{src/pages/discussion.py} file.

The reasoning models incorporate a \texttt{<think>...</think>} mechanism, enabling explicit reasoning steps that are displayed separately from the final response in the user interface.

\section{Agents architecture}

Agents are organised as shown:

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{static/schemas/classDiagramAgents.drawio.png}
    \caption{Agents class diagram}
    \label{fig:agents}
\end{figure}

The \texttt{RetrievalAgent} and \texttt{ReportAgent} both use custom structured output at some point in their execution. They come in the form of pydantic's BaseModels:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\linewidth]{static/schemas/classDiagramStructuredOutputs.drawio.png}
    \label{fig:StructuredOutputs}
\end{figure}

\subsection{Graph State Architecture}

Homer implements a sophisticated state management system using LangGraph's state abstractions. Each agent maintains its specific state structure while sharing common interfaces:

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{static/schemas/classDiagramStates.drawio.png}
    \caption{Agents states}
    \label{fig:states}
\end{figure}

\subsection{State Reducers}

The system implements custom state reducers for complex state transitions:

\begin{lstlisting}[caption={Document state reducer}]
def reduce_docs(existing: Optional[Sequence[Document]], 
                new: Union[Sequence[Document], str, Literal["delete"]]):
    if new == "delete":
        return []
    # Handle various input types and transformations
\end{lstlisting}

\subsection{Memory and Persistence}

\begin{itemize}
    \item \textbf{Conversation Memory}: RetrievalAgent uses SQLite-based checkpointing for conversation persistence
    \item \textbf{Document Storage}: ChromaDB provides persistent vector storage with metadata
    \item \textbf{Session State}: Streamlit manages user session state for UI consistency
\end{itemize}

In a previous version, making the discussions persistent has been considered, but the streamlit interface made switching discussions difficult. You can find some files related to this in \texttt{src/schemas} and \texttt{src/core/database.py}.\\
To enable the persistent database, the value \texttt{":memory:"} from \texttt{src/utils/utils.py, get\_connection} should be changed to the persistent database directory. It will not work out of the box as some functions related to this have been removed of the current release.

\subsection{RetrievalAgent - Conversational RAG Implementation}

The RetrievalAgent implements a stateful conversational retrieval system using LangGraph's state management capabilities. The agent maintains conversation context through an SQLite checkpointer, enabling multi-turn conversations with persistent memory.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\linewidth]{static/schemas/retrievalGraph.drawio.png}
    \caption{Retrieval graph structure}
    \label{fig:retrievalGraph}
\end{figure}

\textbf{Nodes:}\\

\paragraph{Rephrase query}

Generate an optimized search query based on conversation context.  This function analyzes the current conversation state and generates a search query optimized for document retrieval, based on the last interaction.

\begin{lstlisting}[caption={Rephrase query node}]
def rephrase_query(
    state: RetrievalState, *, config: RunnableConfig
) -> Dict[str, Union[str, List]]:

    configuration = Configuration.from_runnable_config(config)
    
    # Load and configure model
    model = load_chat_model(
        model=configuration.query_model, 
        host=configuration.ollama_host
    ).with_structured_output(SearchQuery)
    
    # Prepare message context
    previous_messages = (
        format_messages(state.messages[-3:-1]) 
        if len(state.messages) >= 3 
        else "There were no previous messages."
    )
    
    # Create prompt
    system_prompt = prompts.REPHRASE_QUERY_SYSTEM_PROMPT.format(
        previous_messages= previous_messages,
    )
    user_prompt = state.messages[-1].content
    
    messages = [
        ("human", combine_prompts(system_prompt,user_prompt)),
    ]
    
    # Generate rephrased query
    generated = cast(SearchQuery, model.invoke(messages, config))
    
    retrievalAgentLogger.info(f"Generated query: '{generated.query}'")
    
    return {
        "query": generated.query,
        "retrieved_docs": "delete" if state.retrieved_docs else [],
    }
\end{lstlisting}

\paragraph{Retrieve}

Retrieve relevant documents based on the generated query. This function takes a search query from the state and retrieves the most relevant documents from the indexed document collection using vector similarity search. It uses the configured embedding model to encode the query and find matching documents.

\begin{lstlisting}[caption={Retrieve node}]
def retrieve(
    state: RetrievalState, *, config: RunnableConfig
) -> Dict[str, List[Document]]:

    configuration = Configuration.from_runnable_config(config)
    
    # Load embedding model
    embeddings = load_embedding_model(
        model=configuration.embedding_model, 
        host=configuration.ollama_host
    )

    # Retrieve documents
    with retrieval.make_retriever(embedding_model=embeddings) as retriever:
        response = retriever.invoke(state.query, config)
        
        if response:
            retrievalAgentLogger.info(f"Successfully retrieved {len(response)} documents")
            for doc in response:
                retrievalAgentLogger.debug(f"Document: {doc.page_content} from {doc.metadata.get('source', 'unknown')}\n")
        else:
            retrievalAgentLogger.warning("No documents retrieved for the query")
        
        return {"retrieved_docs": response}
\end{lstlisting}

\paragraph{Respond}

 Generate a conversational response based on retrieved documents and chat history. This function creates a contextual response using the retrieved documents as context, considering the conversation history and any existing conversation summary. It handles message history efficiently by using summaries for older conversations.

\begin{lstlisting}[caption={Respond node}]
def respond(
    state: RetrievalState, *, config: RunnableConfig
) -> Dict[str, Union[List[BaseMessage], List, str]]:

    configuration = Configuration.from_runnable_config(config)
    
    # Load model
    model = load_chat_model(
        model=configuration.response_model, 
        host=configuration.ollama_host
    )
    
    # Prepare context
    previous_messages = ya_format_messages(state.messages[-3:-1] if len(state.messages)>2 else [])
    context_docs = format_docs(state.retrieved_docs) if state.retrieved_docs else ""
    
    # Create prompt
    system_prompt = prompts.RESPONSE_SYSTEM_PROMPT.format(
        context = context_docs,
        summary = state.summary if state.summary else "",
    )

    messages = [
        ("system", system_prompt)
    ] + previous_messages + [
        ("human", state.messages[-1].content)
    ]

    # Generate response
    response = model.invoke(input=messages, config=config)

    return {
        "messages": [response],
        "retrieved_docs": [],
        "query": "",
    }
\end{lstlisting}

\paragraph{Summarize conversation}

Create or extend a conversation summary to manage long chat histories. This function generates a concise summary of recent conversation messages, either creating a new summary or extending an existing one. This helps maintain context while keeping prompt sizes manageable for long conversations.
    
\begin{lstlisting}[caption={Summarize conversation node}]
def summarize_conversation(
    state: RetrievalState, *, config: RunnableConfig
) -> Dict[str, str]:

    configuration = Configuration.from_runnable_config(config)
    
    # Get existing summary
    existing_summary = state.summary if state.summary else ""
    messages_to_summarize = ya_format_messages(state.messages[-6:])  # Last 6 messages
    
    # Determine prompt based on existing summary
    if existing_summary:
        summary_system_prompt = f"""This is summary of the conversation to date:
<summary>
{existing_summary}
</summary>

Extend the summary by taking into account the new messages:"""

    else:
        summary_system_prompt = "Create a summary of the conversation:"

    # Load model
    model = load_chat_model(
        model=configuration.query_model, 
        host=configuration.ollama_host
    )
    
    # Create prompt
    messages = [
        ('system', summary_system_prompt)
    ] + messages_to_summarize

    
    response = model.invoke(messages, config)

    return {"summary": response.content}
\end{lstlisting}

Determine whether conversation summarization should occur. This function implements the logic for deciding when to summarize the conversation based on message count. It triggers summarization every 6 messages to keep conversation context manageable while preserving important information.
    
\begin{lstlisting}[caption={Summarize conditional edge}]
def should_summarize(
    state: RetrievalState, *, config: RunnableConfig
) -> str:
    message_count = len(state.messages)
    should_trigger = message_count % 6 == 0

    if should_trigger:
        return "summarize_conversation"
    else:
        return END
\end{lstlisting}

\subsection{ReportAgent - Structured Document Generation}

The ReportAgent generates comprehensive technical reports through a multi-stage pipeline that creates structured, well-organized documents from the knowledge base.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.2\linewidth]{static/schemas/reportGraph.drawio.png}
    \caption{Report graph structure}
    \label{fig:reportGraph}
\end{figure}

\textbf{Nodes:}\\

\paragraph{Initial retrieval}

Retrieve documents based on the input state to create the outline. This function performs the initial document retrieval using the user's main query to gather relevant context that will inform the outline generation. It serves as the foundation for understanding what information is available for the report.

\begin{lstlisting}[caption={Initial retrieval node}]
def initial_retrieval(
    state: ReportState, *, config: RunnableConfig) -> dict[str, list[Document]]:

    configuration = Configuration.from_runnable_config(config)
    
    # Extract main query from the last message
    main_query = get_message_text(state.messages[-1])

     # Setup retriever with embedding model
    with retrieval.make_retriever(
        embedding_model=load_embedding_model(model=configuration.embedding_model, host=configuration.ollama_host),   
    ) as retriever:
        # Perform document retrieval
        response = retriever.invoke(main_query, config)
        
    return {"retrieved_docs": response}
\end{lstlisting}

\paragraph{Generate outline}

Generate an outline based on the user query and retrieved context. This function creates a structured outline for the report using the initially retrieved documents as context. It uses a language model with structured output to ensure the outline has the proper format and number of sections specified in the report configuration.

\begin{lstlisting}[caption={Initial retrieval node}]
def generate_outline(
    state: ReportState, *, config: RunnableConfig) -> dict[str, Any]:

    configuration = Configuration.from_runnable_config(config)
    
    # Extract query and context information
    main_query = get_message_text(state.messages[-1])
    context_text = "\n\n".join([doc.page_content for doc in state.retrieved_docs])
    
    # Load model with structured output
    model = load_chat_model(model=configuration.report_model, host=configuration.ollama_host).with_structured_output(Outline)
    
    # Create prompt with context and requirements
    system_prompt = prompts.OUTLINE_SYSTEM_PROMPT.format(
        context = format_docs(state.retrieved_docs),
        number_of_parts = configuration.number_of_parts
    )
    user_prompt = state.messages[-1].content
    messages = [
        ("human", combine_prompts(system=system_prompt, user=user_prompt))
    ]

    # Generate outline using structured output
    generated = cast(Outline, model.invoke(messages, config))

     # Initialize report structure and determine writing style label
    style_label = "Technical Report" if configuration.writing_style == "technical" else "General Report"
    report_header = f"{style_label.upper()}\nTITLE: {main_query}\n\n"
    
    return {
        "outlines": generated.entries,
        "current_section_index": 0,
        "report_header": report_header,
    }    
\end{lstlisting}

\paragraph{Retrieve for section node}

    Retrieve documents for the current section being processed. This function performs targeted document retrieval for the specific section currently being processed. It uses the section title as the query to find the most relevant documents for that particular part of the report.

\begin{lstlisting}[caption={Retrieve for section node}]
def retrieve_for_section(
    state: ReportState, *, config: RunnableConfig) -> dict[str, list[Document]]:

    # Validate section state
    if not state.outlines:
        return {"retrieved_docs": []}
    
    if state.current_section_index >= len(state.outlines):
        return {"retrieved_docs": []}
        
    configuration = Configuration.from_runnable_config(config)
    
    current_section = state.outlines[state.current_section_index]

    # Setup retriever and perform document retrieval
    with retrieval.make_retriever(
        embedding_model=load_embedding_model(model=configuration.embedding_model, host=configuration.ollama_host),   
    ) as retriever:
        # Retrieve documents using section title as query
        response = retriever.invoke(current_section, config)
        
    return {"retrieved_docs": response}
\end{lstlisting}

\paragraph{Synthesize section node}

Synthesize raw section content from retrieved documents. This function generates the initial content for the current report section using the documents retrieved specifically for that section. It applies the appropriate writing style (technical or general) and creates comprehensive content that addresses the section topic using the available context.

\begin{lstlisting}[caption={Synthesize section node}]
def synthesize_section(
    state: ReportState, *, config: RunnableConfig) -> dict[str, Any]:

    # Validate section state
    if not state.outlines:
        return {"raw_section_content": ""}

    if state.current_section_index >= len(state.outlines):
        return {"raw_section_content": ""}
        
    configuration = Configuration.from_runnable_config(config)
    
    current_section = state.outlines[state.current_section_index]
    main_query = state.messages[-1].content

    # Load content generation model
    model = load_chat_model(model=configuration.report_model, host=configuration.ollama_host)

    # Select appropriate prompt based on writing style
    prompt = prompts.TECHNICAL_SECTION_SYSTEM_PROMPT if configuration.writing_style == "technical" else prompts.GENERAL_SECTION_SYSTEM_PROMPT
    
    # Format prompt with context and section information
    formatted_prompt = prompt.format(
        context = format_docs(state.retrieved_docs),
        current_section = current_section,
        main_query = main_query
    )
    messages  = [
        ("human", formatted_prompt)
    ]

    # Generate section content
    response = model.invoke(messages, config)
    synthesized_content = get_message_text(response).strip()

    return {"raw_section_content": synthesized_content}
\end{lstlisting}

\paragraph{Review section node}

This function takes the raw content generated in synthesize\_section and applies a review and polishing process to improve quality, coherence, and alignment with the overall report objectives. It creates the final section content and advances the processing to the next section. It creates the final section content and advances the processing to the next section.

\begin{lstlisting}[caption={Review section node}]
def review_section(
    state: ReportState, *, config: RunnableConfig) -> dict[str, Any]:

    # Validate section state
    if not state.outlines:
        return {}

    if state.current_section_index >= len(state.outlines):
        return {}
        
    configuration = Configuration.from_runnable_config(config)
    
    current_section = state.outlines[state.current_section_index]
    main_query = state.messages[-1].content

    # Load review model
    model = load_chat_model(model=configuration.report_model, host=configuration.ollama_host)
    
    # Format review prompt with context
    prompt = prompts.REVIEW_SYSTEM_PROMPT.format(
        main_query = main_query,
        current_section = current_section,
        draft_section = state.raw_section_content,
    )
    messages  =  [
        ("human", prompt)
    ]

    # Generate polished content
    response = model.invoke(messages, config)
    polished_content = get_message_text(response).strip()
    
    # Create final section structure
    final_section = {"title": current_section, "content": polished_content}
    next_index = state.current_section_index + 1
    
    return {
        "report": [final_section],
        "current_section_index": next_index
    }
\end{lstlisting}

The following function implements the control logic for the iterative section processing workflow. It checks if there are more sections to process based on the current section index and the total number of outlined sections.
    
\begin{lstlisting}[caption={Should continue conditional edge}]
def should_continue(state: ReportState, *, config: RunnableConfig):]:

    if not state.outlines:
        return END
    
    if current_index >= len(state.outlines):
        return END
    else:
        return "retrieve_for_section"
\end{lstlisting}

\textbf{Writing Style Support:}
\begin{itemize}
    \item \textbf{Technical Style}: Detailed technical analysis with precise values and specifications
    \item \textbf{General Style}: Accessible explanations suitable for broader audiences
\end{itemize}

\subsection{IndexAgent - Document Ingestion Pipeline}

The IndexAgent manages the document ingestion pipeline, transforming PDF documents into searchable vector representations stored in the ChromaDB vectorstore.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{static/schemas/indexGraph.drawio.png}
    \caption{Index graph structure}
    \label{fig:indexGraph}
\end{figure}

\textbf{Nodes:}\\

\paragraph{Parse pdf node}

This function scans a directory for PDF files, loads them using PyMuPDFLoader, splits the content into smaller chunks using RecursiveCharacterTextSplitter, and returns the processed documents. It filters out already processed files to avoid duplicates.

\begin{lstlisting}[caption={Parse pdf node}]
def parse_pdfs(
    state: InputIndexState, *, config: Optional[RunnableConfig] = None
) -> dict[str, str]:

    configuration = Configuration.from_runnable_config(config)
    
    path = Path(state.path)
    if not path.is_dir():
        raise FileNotFoundError(f"Directory not found: {state.path}")

    documents = []
    
    # Get new PDF files (excluding already processed ones)
    pdf_files = remove_duplicates(
        base=retrieval.get_existing_documents(),
        new=[str(p) for p in list(path.glob("*.pdf"))]
    )
    
    if not pdf_files:
        return {"docs": documents}
    
    #embeddings = load_embedding_model(model=Configuration.embedding_model, host=Configuration.ollama_host)

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=4000,
        chunk_overlap=200,
        length_function=len,
        is_separator_regex=False
    )

    # Process each PDF file
    for pdf_file in tqdm(pdf_files, desc="Loading files..."):                
        # Load the file into a Document object
        if configuration.ocr:
            logger.debug("using server parser")
            loader = VisionLoader(
                file_path=str(pdf_file),
                mode = 'single',
                ollama_base_url= configuration.ollama_host,
                ollama_model=configuration.vision_model,
            )
        else:
            logger.debug("distant client not found, falling back to local parser")
            loader = PyMuPDFLoader(
                file_path=str(pdf_file),
                extract_tables='markdown',
                mode= "single"
            )
        
        # Split the Document content into smaller chunks
        document = text_splitter.split_documents(loader.load())
        #ensure metadata
        document[0].metadata={"source": pdf_file}
        # Add them to the list of Documents
        documents.extend(document)


    return {"docs": documents}
\end{lstlisting}

The following function is a conditional edge that skips the \texttt{index\_docs} node in case there are no documents to index.

\begin{lstlisting}[caption={Should index conditional edge}]
should_index(state: IndexState, *, config: RunnableConfig) -> str:

    if not state.docs:
        logger.info("No documents to index, ending workflow")
        return END

    return "index_docs"
\end{lstlisting}

\paragraph{Index documents node}

This function takes documents from the state, processes them in batches, and adds them to the configured retriever's index using the specified embedding model. After successful indexing, it signals for documents to be removed from the state.

\begin{lstlisting}[caption={Index doc node}]
def index_docs(
    state: IndexState, *, config: Optional[RunnableConfig] = None
) -> dict[str, str]:

    configuration = Configuration.from_runnable_config(config)
    
    # Prepare document batches
    documents_batch = make_batch(obj=state.docs, size= 20)

    # Index documents using the retriever
    with retrieval.make_retriever(
        embedding_model=load_embedding_model(model=configuration.embedding_model)
    ) as retriever:
        
        for i, batch in enumerate(tqdm(documents_batch, desc="Adding document batch..."), 1):
            retriever.add_documents(batch)

    return {"docs": "delete"} # To clear the value of the current thread
\end{lstlisting}

\textbf{Chunking Strategy}\\

The system supports two chunking approaches:

\begin{itemize}
    \item \textbf{Recursive Character Splitting}: Fixed-size chunks with overlap (4000 characters, 200 overlap)
    \item \textbf{Semantic Chunking}: Meaning-based division using embedding models (commented out for performance)
\end{itemize}

\begin{lstlisting}[caption={Text splitting configuration}]
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=4000,
    chunk_overlap=200,
    length_function=len,
    is_separator_regex=False
)
\end{lstlisting}

\textbf{Batch Processing:}
Documents are processed in batches of 20 to limit memory usage and provide progress feedback:

\begin{lstlisting}[caption={Batch processing implementation}]
documents_batch = make_batch(obj=state.docs, size=20)
for batch in tqdm(documents_batch, desc="Adding document batch..."):
    retriever.add_documents(batch)
\end{lstlisting}

\section{Retrieval System}

\subsection{Vector Store Management}

Homer utilizes ChromaDB as its primary vector store, providing persistent storage with cosine similarity search capabilities. The retrieval system implements a context manager pattern for resource management:

\begin{lstlisting}[caption={Vector store retriever implementation}]
@contextmanager
def make_retriever(embedding_model: Embeddings, **kwargs):
    vector_store = Chroma(
        collection_name=_COLLECTION,
        collection_metadata={"hnsw:space": "cosine"},
        embedding_function=embedding_model,
        persist_directory=VECTORSTORE_DIR,
    )
    yield vector_store.as_retriever(
        search_type="similarity_score_threshold", 
        search_kwargs={"k": 5, "score_threshold": 0.4}
    )
\end{lstlisting}

\subsection{Search Configuration}

The retrieval system supports multiple search strategies:

\begin{itemize}
    \item \textbf{Similarity Search}: Standard cosine similarity with configurable top-k results
    \item \textbf{Similarity Score Threshold}: Filters results below a minimum relevance threshold
    \item \textbf{Maximum Marginal Relevance (MMR)}: Balances relevance and diversity in results
\end{itemize}

\textbf{Default Parameters:}
\begin{itemize}
    \item \textbf{Top-k}: 5 documents per query
    \item \textbf{Score Threshold}: 0.4 minimum similarity
    \item \textbf{Collection}: "HOMER" with cosine similarity space
\end{itemize}

\subsection{Document Lifecycle Management}

The system provides comprehensive document lifecycle management through utility functions:

\begin{lstlisting}[caption={Document management functions}]
def get_existing_documents() -> list[str]:
    """Returns list of all indexed document sources"""
    
def delete_documents(docs: str | list[str]):
    """Removes documents by source filename"""
\end{lstlisting}

This enables users to manage their document collection, remove outdated content, and track indexing status through the web interface.\\

\paragraph{Get existing documents.}
The function \texttt{get\_existing\_documents} is used to list all the documents currently indexed in the ChromaDB vector store. More specifically, it returns the unique source file names associated with the stored documents. These source names are extracted from the metadata that is attached to each document. \\
This part of the code connects to the ChromaDB client and retrieves (or create if it does not exist) the "HOMER" collection. 
\begin{lstlisting}
client = get_chroma_client()
collection = client.get_or_create_collection(name=_COLLECTION, metadata=_COLLECTION_METADATA)
\end{lstlisting}
This line fetches all documents stored in the collection but includes only their metadata, not the actual content or embeddings
\begin{lstlisting}
results = collection.get(include=["metadatas"])
\end{lstlisting}

This block loops through the metadata and collects all values from the \texttt{"source"} field, which usually corresponds to the original file name. A set is used to ensure uniqueness (no duplicates).
\begin{lstlisting}
sources = set()
for metadata in results["metadatas"]:
if metadata and "source" in metadata.keys():
    sources.add(metadata["source"])
\end{lstlisting}

\paragraph{Delete documents.}
The function \texttt{delete\_documents} is used to remove one or multiple documents from the ChromaDB collection based on their source filename. This is useful for deleting outdated or incorrect files that were previously indexed.

Before deletion, the function ensures that the input is a list, even if the user passed a single string. This provides uniform handling.
\begin{lstlisting}
if isinstance(docs, str):
docs = [docs]
\end{lstlisting}

Then, for each source name in the list, the function deletes all documents that have a matching \texttt{"source"} field in their metadata.
\begin{lstlisting}
for doc_source in docs:
collection.delete(where={"source": doc_source})
\end{lstlisting}


\subsection{Experimental Support: Hybrid Retrieval with Milvus}

Homer also included an \textbf{experimental} backend using \textbf{Milvus Standalone}, a high-performance, embedded vector database available on Linux and macOS. Although not yet part of the stable release, this prototype demonstrates how Milvus can support hybrid search capabilities.

Milvus lite does not support hybrid-search and is not available on Windows; hence, ChromaDB was chosen. Milvus would be relevant in the case of a shared database between users, that would be running on a server.\\
In the example below, we are building a retriever that allows hybrid-search between Dense (HNSW with cosine metric) and BM25 sparse search.

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={Milvus hybrid retriever}]
from langchain_milvus import Milvus, BM25BuiltInFunction
from langchain_core.embeddings import Embeddings

@contextmanager
def make_retriever(embedding_model: Embeddings, **kwargs):
    vector_store = Milvus(
        embedding_function=embedding_model,
        builtin_function=BM25BuiltInFunction(),  # Enable BM25 for sparse vectors
        vector_field=["dense", "sparse"],  # Dense embeddings + sparse BM25
        collection_name=_COLLECTION,
        connection_args={
            "uri": VECTORSTORE_DIR,  # Local file-based storage like ChromaDB
        },
        index_params=[
            {"metric_type": "COSINE", "index_type": "HNSW"},   # Dense index
            {"metric_type": "BM25", "index_type": "AUTOINDEX"} # Sparse index
        ]
        consistency_level="Strong",
        drop_old=False,  # Don't drop existing collection
    )
    
    # Configure retriever with hybrid search and score threshold
    yield vector_store.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={
            "k": 5,
            "score_threshold": 0.4,
            # For hybrid search, you can also specify:
            "ranker_type": "weighted",  # Use weighted ranking
            "ranker_params": {"weights": [0.7, 0.3]},  # Dense: 0.7, Sparse: 0.3
        }
    )
\end{lstlisting}
\end{minipage}

\paragraph{Advantages of Milvus.}
\begin{itemize}
    \item \textbf{Quick Setup:} Fully embedded—no Docker, no remote server
    \item \textbf{Hybrid Search:} Combines semantic and keyword-based retrieval
    \item \textbf{Scalable:} Designed to scale up to billions of vectors
    \item \textbf{Modular Integration:} Compatible with Homer's existing architecture
\end{itemize}

This experimental integration highlights the extensibility of Homer's architecture. In the future, a parallel \texttt{server\_retrieve} node could be a possibility, to search for additional context in another knowledge base.

\section{Error Handling and Resilience}

\subsection{Comprehensive Error Management}

Each component implements robust error handling with degradation:

\begin{lstlisting}[caption={Error handling pattern}]
try:
    # Main processing logic
    response = model.invoke(messages, config)
    logger.info("Operation completed successfully")
    return {"result": response}
except Exception as e:
    logger.error(f"Error in operation: {str(e)}")
    # Fallback mechanism
    return {"result": fallback_response}
\end{lstlisting}

\subsection{Logging Architecture}

The system implements structured logging with configurable levels:

\begin{lstlisting}[caption={Logging implementation}]
setup_logging("INFO")

# Usage across components
logger = get_logger(__name__)
\end{lstlisting}

\textbf{Log Levels and Usage:}
\begin{itemize}
    \item \textbf{INFO}: System status, configuration changes, major operations
    \item \textbf{DEBUG}: Detailed operation traces, parameter values, intermediate states
    \item \textbf{WARNING}: Non-fatal issues, fallback activations
    \item \textbf{ERROR}: Failed operations, exceptions with context
\end{itemize}

\subsection{Caching Strategy}


\begin{itemize}
    \item \textbf{Model Loading}: Models are cached within session state to avoid recompilation
    \item \textbf{Vector Store}: ChromaDB provides built-in caching for embedding lookups
    \item \textbf{Configuration}: Static configuration loading with dynamic host resolution
\end{itemize}

This architecture ensures optimal performance while maintaining system reliability and user experience quality.




\chapter{Vision Parser}

\section{VisionLoader}

Homer implements a dual-path document parsing system supporting both traditional text extraction and advanced vision-based processing. The parser architecture is designed around the \texttt{VisionLoader} class, which extends LangChain's \texttt{BaseLoader} interface.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.55\linewidth]{static/schemas/classDiagramVisionLoader.drawio.png}
    \caption{Class diagram of the vision loader}
    \label{fig:VisionLoader}
\end{figure}

The vision parser is a loader based on Langchain's BaseLoader. It leverages a custom vision parser pipeline described later.\\

\begin{minipage}{\linewidth}
\begin{lstlisting}[caption={VisionLoader class structure}]
class VisionLoader(BaseLoader):
    def __init__(self, file_path: str, ollama_model: str, 
                 ollama_base_url: str, mode: Literal['single','page'] = 'page'):
        # Configuration and pipeline initialization
        self.pipeline = parser.PDFToMarkdownPipeline(
            ollama_model=ollama_model,
            ollama_base_url=ollama_base_url,
            enable_validation=False,
            dpi=400
        )
\end{lstlisting}
\end{minipage}

\section{Multi-Modal Parsing Architecture}

The multi-modal parser uses an Ollama LLVM to extract content from PDF documents, particularly effective for documents containing complex layouts, tables, mathematical formulas, and embedded images. In the current release, we use \texttt{Qwen2.5vl}. The implementation uses the following pipeline:

\begin{enumerate}
    \item \textbf{Page Rasterization}: PDF pages are converted to high-resolution PNG images (default 400 DPI)
    \item \textbf{Vision Model Inference}: Each page image is processed by the configured vision model
    \item \textbf{Content Extraction}: The model outputs markdown-formatted text preserving structure
    \item \textbf{Content Validation}: The model output is compared to the \texttt{fitz} text extraction to ensure that there are no significant data loss. (Not used in the current release)
\end{enumerate}

\begin{figure}[H]
    \centering
    \includegraphics[width=\linewidth]{static/schemas/classDiagramOllamaVisionParser.drawio.png}
    \caption{Class diagram of the vision parser}
    \label{fig:VisionParser}
\end{figure}

The vision parser pipeline has been built using \texttt{Claude Sonnet 4} extensively.

\section{Text Validation System}

The parser includes a comprehensive validation system (\texttt{src/parser/validation.py}) that compares vision model output against traditional PyMuPDF text extraction. This dual-validation approach ensures accuracy and identifies potential extraction errors.

\textbf{Validation Metrics:}
\begin{itemize}
    \item \textbf{Word Overlap Ratio}: Percentage of words correctly extracted
    \item \textbf{Character Overlap Ratio}: Character-level accuracy using longest common subsequence
    \item \textbf{Sentence Overlap Ratio}: Structural preservation of sentence boundaries
    \item \textbf{Semantic Similarity}: Overall content similarity using normalized edit distance
\end{itemize}

The validation system calculates a weighted overall score:
\begin{equation}
V_{\text{score}} = 0.4 \times \text{Word Overlap} + 0.2 \times \text{Char Overlap} + 0.3 \times \text{Sentence Overlap} + 0.1 \times \text{Semantic Similarity}
\end{equation}

\subsection{Fallback Mechanism}

When vision parsing is disabled in the runnable config, the system falls back to PyMuPDF text extraction with table markdown support:

\begin{lstlisting}[caption={Parser fallback mechanism}]
if configuration.ocr:
    loader = VisionLoader(file_path=str(pdf_file), ...)
else:
    loader = PyMuPDFLoader(
        file_path=str(pdf_file),
        extract_tables='markdown',
        mode="single"
    )
\end{lstlisting}

This ensures consistent functionality across different deployment scenarios while optimizing resource utilization.

\chapter{Setup}

\section{Homer Git}

To download the project files to your computer, open your terminal or command prompt, and type the following command line that will create a folder with all the files from the project. 

\begin{lstlisting}
cd your\directory\
git clone https://github.com/flowbrg/homer.git
cd homer
\end{lstlisting}

\section{Python}

The application requires Python version \textbf{3.10} or later. To verify your Python version you can use : 
\begin{lstlisting}
python --version
\end{lstlisting}


\section{Virtual Environment}

We highly recommend you to create a \textbf{Python virtual environment}. It's an isolated sapce for a project's Python dependencies. It allows us to install and manage packages without affecting other projects or the system-wide Python setup. We use it for \textbf{isolation} (keeps project dependencies separate), \textbf{consistency} (ensures everyone uses the same packages) and \textbf{clean development} (avoids conflicts between different projects). 
Here is how to setup : 
\begin{lstlisting}
python -m venv .venv #creates the venv named .venv
.\.venv\Scripts\Activate.ps1 #activates the venv
\end{lstlisting}

Then you have to install the required packages in the venv using the following line: 

\begin{lstlisting}
pip install -r requirements.txt
\end{lstlisting}

\section{Start the Streamlit app}

To launch the HOMER application using Streamlit, simply execute the \texttt{main.py} script with the following command:

\begin{lstlisting}
python main.py
\end{lstlisting}

This will start the Streamlit development server and make the application accessible at the default address: \texttt{http://localhost:8501} in your web browser


\section{Server execution}

In addition to local execution, Homer also supports running Ollama models on a remote server. This enables faster response times and improves scalability, especially when deploying large language models (LLMs) that require substantial memory and compute resources.

\paragraph{Step 1: Configure the SSH Tunnel (using PuTTY)}
\begin{itemize}
    \item Open PuTTY and enter the IP address or hostname of the remote server.
    \item In the sidebar, navigate to \texttt{Connection > SSH > Tunnels}.
    \item In the \textbf{Source port}, enter \texttt{6780}.
    \item In the \textbf{Destination}, enter \texttt{localhost:6780}.
    \item Click \texttt{Add}.
    \item Go back to \texttt{Session}, save the session configuration, and click \texttt{Open} to start the tunnel.
\end{itemize}


\paragraph{Step 2 : Server Setup}

To host an Ollama model on a remote server, follow these steps:

\begin{enumerate}
    \item \textbf{Install Ollama} on the remote machine by following the official installation instructions: \url{https://ollama.com/download}.
    
    \item \textbf{Start the Ollama service} on the remote server using the tunnel port 6780 :
    \begin{lstlisting}
OLLAMA_HOST=0.0.0.0:6780 ollama serve
    \end{lstlisting}
    
    \item \textbf{Pull the desired model} (e.g., \texttt{llama3}) :
    \begin{lstlisting}
OLLAMA_HOST=0.0.0.0:6780 pull llama3
    \end{lstlisting}
    
\end{enumerate}


\paragraph{Step 3 : Client Setup}

On the client side (where the HOMER application runs), you can establish a secure SSH tunnel to forward the remote port to your local machine. This is particularly useful for development environments or when working behind firewalls.
\newline
\newline
You can change the \texttt{OLLAMA\_CLIENT} address in the python file \texttt{./src/constant.py} where you will find the following line : 

\begin{lstlisting}
OLLAMA_CLIENT = "http://127.0.0.1:6780/" #change 6780 if you're using an other tunnel port
\end{lstlisting}

This tells the application to send all model-related queries to the remote server rather than to the local instance.


\chapter{How to use Homer}

\section{Home page}

Once the application is launched (refer to Chapter 4 for details), you will be directed to the \textbf{Home} page. A navigation sidebar appears on the left, allowing users to switch between different sections of the application.
\\
\\
To begin, you must upload your documents by navigating to the \textbf{Documents} page. This step is essential, as the agent's responses are strictly based on the content of the uploaded documents.

\section{Documents page}

The \textbf{Documents} page is where you upload the PDF files you want the application to work with. These documents form the knowledge base that \textbf{Homer} will use to answer your questions or generate reports. If you don't upload any documents, the system will have no information to work from.


\subsection*{Step-by-Step Guide}

\begin{enumerate}
    \item \textbf{Navigate to the Documents Page} \\
    Use the sidebar on the left to access the \texttt{Documents} section.

    \item \textbf{Enable Server Execution (Recommended)} \\
    In the sidebar, activate the toggle labeled \texttt{Server Execution}. This enables the application to process and index documents using a \textbf{remote server}, rather than your local machine. \\
    \textit{Why this matters:} Remote indexing is significantly \textbf{faster and more efficient}, especially when working with large documents or using advanced features like vision parsing.

    \item \textbf{(Optional) Enable Vision Parser} \\
    Below the server execution toggle, you can enable the \texttt{Vision Parser}. This feature allows the application to extract content from \textbf{scanned documents or images} embedded in PDFs. \\
    \textit{Note:} This option requires more computing power and works best when server execution is turned on.

    \item \textbf{Upload Your PDF Files} \\
    Use the upload box to select one or more PDF files from your computer. Once selected, click the \texttt{Upload} button to send them to the system. A confirmation message will appear when the process is complete.

    \item \textbf{Manage Your Documents} \\
    Uploaded documents are listed below the upload section. Each file can be deleted individually using the \texttt{Delete} button next to it.

    \item \textbf{Reset the Document Database (Optional)} \\
    Click the \texttt{Reset Database} button to remove all uploaded documents and start fresh.
\end{enumerate}

\subsection*{Important Notes}
\begin{itemize}
    \item Only \textbf{PDF files} are supported.
    \item The AI can only respond based on the documents you provide; no external data is used.
    \item \textbf{Enabling Server Execution} is highly recommended for improved speed and reliability.
    \item Use the \texttt{Vision Parser} for scanned or image-based documents.
\end{itemize}



\section{Configuration Page}

The \textbf{Configuration} page allows you to customize how the application behaves when processing and analyzing your documents. This includes setting which models are used for different tasks and specifying the server connection for remote processing.

\subsection*{Purpose of This Page}

This page is intended for adjusting key settings that control how documents are indexed and how responses are generated. While default values are already provided, this page gives you flexibility if you want to update the models or connect to a different server.

\subsection*{Step-by-Step Guide}

\begin{enumerate}
    \item \textbf{Access the Configuration Page} \\
    From the sidebar, click on the \texttt{Configuration} section to open the settings editor.

    \item \textbf{Edit the Ollama Host (Server URL)} \\
    The first field allows you to change the URL of the server (called the \texttt{Ollama Host}) that performs document indexing and answering tasks.
    \\
A message will appear to confirm whether the URL format is valid.
   

    \item \textbf{Configure Models} \\
    You can modify the models used by the system. There are different models for different modes:
    \begin{itemize}
        \item \textbf{Local Reasoning Model}: Used for \textbf{Discussion} if you enable \textbf{Reasoning model}.
        \item \textbf{Local Standard Model}: Used for \textbf{Discussion} if you disable \textbf{Reasoning model}.
        \item \textbf{Server Standard Model}: Used for \textbf{Discussion} if you disable \textbf{Reasoning model} using a server. 
        \item \textbf{Server Reasoning Model}: Used for \textbf{Discussion} if you enable \textbf{Reasoning model} using a server. 
        \item \textbf{Vision Model}: Used to parse PDFs that are converted into images (requires server execution).
    \end{itemize}

    \item \textbf{Save Your Settings} \\
    After adjusting the fields as needed, click the \texttt{Save Configuration} button. The new settings will be applied immediately, and a success message will appear.
\end{enumerate}

\section{Models Page}

The \textbf{Model} page allows you to download and manage models from Ollama. These models are required for the application to perform tasks such as document embedding and question answering.

\subsection*{Purpose of This Page}

Before the application can run operations, the required models must be downloaded (also known as "pulled") from the \href{https://ollama.com/library}{Ollama model hub}. This page provides a simple interface to do that, whether you're running the application locally or using a remote server.

\subsection*{Step-by-Step Guide}

\begin{enumerate}
    \item \textbf{Enable Server Execution (Optional)} \\
    In the sidebar, you will find a toggle labeled \texttt{Server Execution}. If enabled, the model will be downloaded to the remote server. Otherwise, it will be pulled to your local machine. \\

    \item \textbf{Enter the Model Name} \\
    In the main interface, you’ll see a text box labeled \texttt{Enter the model you want to pull}. Here, you must type the exact name of the model you want to use. \\
    \textit{Note: Model names must match exactly as listed on the Ollama website. For example, \texttt{llama3}, \texttt{gemma:2b}, etc.}

    \item \textbf{Pull the Model} \\
    After entering the model name, click the \texttt{Pull} button. The system will:
    \begin{itemize}
        \item Check if the model already exists.
        \item If not, download it and show a progress bar.
        \item Notify you when the process is complete.
    \end{itemize}

    \item \textbf{View Available Models} \\
    At the bottom of the page, you can see a list of all models that are currently available on the selected host (either local or server if enabled).
\end{enumerate}
\newpage
\section{Discussion Page}



The \textbf{Discussion} page provides a chat interface where you can interact with the agent based on your uploaded documents. This is the main interface for asking questions and getting detailed answers, including the agent's reasoning process if enabled.


\subsection*{Step-by-Step Guide}

\begin{enumerate}
    \item \textbf{Navigate to the Discussion Page} \\
    Use the sidebar to select the \texttt{Discussion} section.

    \item \textbf{(Optional) Enable Server Execution} \\
    In the sidebar, you will find a toggle labeled \texttt{Server Execution}.
    \begin{itemize}
        \item When enabled, responses are generated using a remote server, which is generally faster and more powerful.
        \item When disabled, everything runs locally on your computer.
    \end{itemize}

    \item \textbf{(Optional) Enable Reasoning Model} \\
    You can toggle between a standard model and a reasoning model.
    \begin{itemize}
        \item The reasoning model includes a visible "thinking" process before producing the final answer.
        \item The standard model gives a direct answer without explanation.
    \end{itemize}

    \item \textbf{Enter Your Query} \\
    At the bottom of the chat, type your query into the input field and press \texttt{Enter}.
    \begin{itemize}
        \item Your query will immediately appear in the chat window.
        \item The agent will begin typing its response in real time.
    \end{itemize}

    \item \textbf{View the Answer and Thinking (If Enabled)} \\
    If you enabled the reasoning model, you will see an expandable section labeled \texttt{Show Thinking} containing the model’s intermediate steps or explanation before answering.
\end{enumerate}

\subsection*{Other Features}

\begin{itemize}
    \item \textbf{Conversation Threads}: The system keeps track of your conversation history within a session.
    \item \textbf{Model Selection Feedback}: The sidebar shows which model is currently being used.
    \item \textbf{Server Connection Status}: The sidebar also displays the active server used to process queries.
\end{itemize}


\section{Report Page}

The \textbf{Report} page allows users to generate structured reports based on the documents they have uploaded.

\subsection*{Purpose of This Page}

This interface enables the generation of a detailed report by interacting with the agent. The report is created from the uploaded documents and can be customized according to the user’s needs through several sidebar options.

\subsection*{Step-by-Step Guide}


\begin{enumerate}
    \item \textbf{Navigate to the Report Page} \\
    Use the sidebar to access the \texttt{Report} section.

    \item \textbf{Configure Report Settings}
    
    \begin{itemize}
        \item \textbf{Model} \\
        If you want to change the model that writes the report you should go to \\\texttt{./src/core/configuration.py} and then find this : 
        \begin{lstlisting}
report_model: str = field(
        default = "gemma3:4b",   #change this value,
    )
\end{lstlisting}
We highly recommend you not to use thinking model for this task. 
        


      
        \item \textbf{Number of Report Sections} \\
        Choose how many sections you want in your report. This allows the report to be structured into multiple logical parts.

        \item \textbf{Documents per Section} \\
        Define how many relevant documents (chunks) should be retrieved and analyzed for each part of the report. Increasing this number may lead to more detailed content but less performance (default is 4), while a lower number can make each section more concise.
        
        \item \textbf{Server Execution (Optional)} \\
        Toggle the \texttt{Server Execution} option if you want the report to be generated using a remote server for better performance.
        

    \item \textbf{Enter Your Report Topic} \\
    In the main area, you will find a text box where you can describe what your report should be about. Be as specific as possible to get better results.


    \item \textbf{Download the Report} \\
    Once the report is generated, you can download the PDF file on your personal computer. 
     \end{itemize}
\end{enumerate}



\chapter{Next steps - Future development}

\section{.exe file}

To further simplify the deployment process, we plan to provide a standalone executable file (\texttt{.exe}) that allows users to run the application without requiring a Python environment or any manual setup.

\subsection*{Current Limitation}

At this stage, we do not have the necessary permissions to generate or execute such a file on SCK CEN-managed computers. Specifically, creating executables using tools like PyInstaller requires administrative rights or ICT approval, which we currently lack. An official request must be made to ICT to proceed with this compilation on internal machines.

\subsection*{Proposed Solution}

The idea is to use the \textbf{PyInstaller} library to convert the Python-based Streamlit application into a self-contained executable. This would eliminate the need for users to manually install Python, create a virtual environment, or install packages.

\subsection*{Benefits of a Standalone Executable}

\begin{itemize}
    \item No Python installation required
    \item No command-line interaction for the user
    \item Compatible with environments where dependency installation is restricted
    \item Simplified deployment for end users or clients
\end{itemize}

\subsection*{Automation of Environment Setup}

A shell script has already been prepared that automates the setup:
\begin{itemize}
    \item Creates and activates a virtual environment
    \item Installs all packages from \texttt{requirements.txt}
    \item Launches the Streamlit app
\end{itemize}

\subsection*{Code to Compile}


\vspace{0.5em}
\begin{lstlisting}[caption={setup.py file in the main root to compile}]
import os
import subprocess

# Paths 
project_dir = r"C:\Users\mdlbarre\Documents\homer-main" #update this
venv_dir = os.path.join(project_dir, "venv")
requirements_path = os.path.join(project_dir, "requirements.txt")
main_script = os.path.join(project_dir, "main.py")

# Create the venv
subprocess.call(f'python -m venv "{venv_dir}"', shell=True)

# Activate the venv and run commands
activate_cmd = os.path.join(venv_dir, "Scripts", "activate.bat")

# Combine all into a single command string
commands = f'''
call "{activate_cmd}"
pip install -r "{requirements_path}"
python "{main_script}"
'''

# Run the above in CMD
subprocess.call(commands, shell=True)
\end{lstlisting}

Then you have to run the following command line in the terminal : 

\begin{lstlisting}
pyinstaller setup.py 
\end{lstlisting}

The .exe file will be in the \texttt{dist} folder.



\section{Client–Server Setup with L6 and SSH Tunneling}

A potential enhancement under consideration is to extend the architecture so that the Streamlit application runs locally on the user's L6 machine while offloading model inference to a remote server through an SSH tunnel. This setup would allow users to benefit from the performance of the server-side models without deploying or running heavy computations locally.

\subsection*{Proposed Design}

\begin{itemize}
    \item \textbf{L6 as a Client:} The application interface would execute on L6, acting as a lightweight frontend.
    \item \textbf{Remote Server with Ollama:} The Ollama service would run on a remote compute server (e.g., a cluster), which hosts large language models.
    \item \textbf{SSH Tunnel:} A secure tunnel would forward API calls from the local machine to the server, making the remote service accessible as if it were local.
\end{itemize}

\subsection*{Advantages}

\begin{itemize}
    \item Allows users to run the application without requiring local installation of large models or GPUs.
    \item Offers improved scalability and model performance by using dedicated hardware.
    \item You can switch off your personal computer while it's running in the background. 
\end{itemize}

\section{Agentic RAG}

Models like DeepSeek R1 and Qwen3 demonstrate sophisticated tool-use capabilities that enable more autonomous operation compared to traditional pipeline-based approaches like the Retrieval Graph currently implemented in our system. These agentic models can dynamically determine when and how to retrieve information based on query context and their reasoning process.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.3\linewidth]{static/schemas/agenticRAG.png}
    \caption{Agentic RAG graph}
    \label{arag}
\end{figure}

\subsection*{Tool integration}
This tool abstraction allows the agentic model to determine when retrieval is necessary, formulate appropriate search queries autonomously, and integrate retrieved information into its reasoning chain. 
\begin{lstlisting}
@tool
def search_for_context(query:str, source: Optional[str], config: RunnableConfig):
    configuration = Configuration.from_runnable_config(config)
 
    embeddings = load_embedding_model(model=configuration.embedding_model)
    with make_retriever(embedding_model=embeddings) as retriever:
        results = retriever.invoke(query)
    if not results:
        return "No relevant documents found."
    return format_docs(results)
\end{lstlisting}


\section{Future Integration within the SCK CEN Infrastructure}

Homer is currently a functional prototype developed to demonstrate AI-assisted document analysis using large language models (LLMs). The next logical step is to integrate this tool within the internal infrastructure of SCK CEN.

\subsection*{Integration Objectives}

Integrating Homer into the SCK CEN network will enable:

\begin{itemize}
    \item Secure and compliant document processing within the protected internal environment.
    \item Streamlined access to internal resources such as document repositories, identity management systems, and network storage.
\end{itemize}

\subsection*{Current Limitations}

At this stage, Homer has been developed and tested outside the SCK CEN production environment. Integration requires:

\begin{itemize}
    \item Approval and support from ICT services.
    \item Validation of security protocols, especially for components involving LLMs and SSH tunnels.
    \item Adaptation of the current prototype to comply with internal software deployment standards and access control policies.
\end{itemize}


\subsection*{Continuous Evolution of A.I.} 
Artificial Intelligence is evolving at a rapid pace, with new models, tools, and capabilities emerging frequently. To ensure that SCK CEN remains at the forefront of this technological shift and can fully leverage AI advancements in a secure and efficient manner, it is essential to establish a dedicated AI team. This team would be responsible for monitoring AI trends, updating internal tools such as HOMER, evaluating new model capabilities, and ensuring safe and ethical deployment aligned with the organization's objectives.



\begin{thebibliography}{9}

\bibitem{berge2025specifications}
Florent Bergé \& Mathieu de la Barre, 
\textit{Homer specifications}, 
SCK CEN, 05/2025

\bibitem{aliouat2025rag}
Raouf Aliouat, Rob Baby, Prabal Deb, Chad Kittel, Ritesh Modi, Ryan Pfalz \& Randy Thurman, 
\textit{Design and develop a RAG solution}, 
Microsoft Azure, 01/09/2025

\bibitem{berge2025hybrid}
Florent Bergé \& Mathieu de la Barre, 
\textit{Hybrid search for milvus}, 
SCK CEN, 09/05/2025

\end{thebibliography}

\end{document}